<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<base href="https://engineering.purdue.edu/477grp6/" />

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
			<h2>Progress Report for Zach Ghera</h2>
			
			<h4>Week 2</h4>
			<p>
				<b>Date:</b> 9/3<br>
				<b>Total hours:</b> ~8 hours<br>
				<b>Description of design efforts:</b><br>
				I spent the majority of this week doing research on the feasibility of
				hardware/modules that we need for our system. Please see a summary of each
				progress item below:
				<ul>
					<li>
						<b>Determined the hardware necessary for wireless communication.</b>
						<br>
						The first step I took was researching BLE versus alternatives
						such as RF (specifically ZigBee) to see which would be a more
						viable for our project. More specifically, the choosen wireless
						communication method needs to have:
						<ul>
							<li>Low power consumption and (ideally) 1.8V source voltage</li>
							<li>Low latency</li>
							<li>Low cost</li>
						</ul>
						In addition to looking at some of the most popular (available)
						chips for both 
						<a href="https://www.digikey.jp/products/en/rf-if-and-rfid/rf-transceiver-modules-and-modems/872?k=&pkeyword=xbee&sv=0&pv1883=320634&sf=0&FV=1989%7C0%2C-8%7C872&quantity=&ColumnSort=0&page=1&stock=1&pageSize=25">
							BLE 
						</a>
						and 
						<a href="https://www.digikey.jp/products/en/rf-if-and-rfid/rf-transceiver-modules-and-modems/872?FV=1989%7C0%2C-8%7C872%2C578%7C423589&quantity=0&ColumnSort=0&page=1&stock=1&pageSize=25&pkeyword=xbee">
							RF/Zigbee
						</a>
						, one valuable source to help inform the decision was 
						<a href="https://www.researchgate.net/publication/271327094_On_the_Potential_of_Bluetooth_Low_Energy_Technology_in_Vehicular_Applications">
							this paper on the Potential of Bluetooth Low Energy
							Technology in Vehicular Applications
						</a>
						. As summarized in the Comparison between BLE and ZigBee section,
						BLE offers a faster data rate, lower latency, lower cost, and
						more robust in the presence of strong WiFi interference.
						Therefore, I recommended that we proceed with buying a BLE chip.
						<br>
						I then did some research to get a sense of what are some of the
						most popular BLE controllers. 
						<a href="https://developex.com/blog/most-popular-ble-controllers/">
							This blog
						</a>
						was one of many websites that I referenced alongside filtering
						through different bluetooth chips on the digikey website. After
						some review of the available parts, I chose the 
						<a href="https://www.digikey.jp/product-detail/en/jorjin-technologies-inc/ZB7412-00/2393-ZB7412-00CT-ND/12142962">
						ZB7412 module 
						</a>
						based on the TI CC26xx BLE controller. The main primary rationale
						behind this choice was that it had the second lowest current
						consumption (behind the DA 14580 which had no available BLE modules).

					</li>
					<li>
						<b>Confirmed ability to disable IMU (
						<a href="https://www.digikey.com/en/products/detail/bosch-sensortec/BHI160B/9674245?s=N4IgTCBcDaIBxjgWgIwAY4E5VIHIBEQBdAXyA">
							BHI160B
						</a>
						) fusion core.</b>
						<br>
						After the team ordered a few IMUs, I found that this particular IMU
						had an onboard "fusion core" that allows a client to read from
						"virtual sensors." These sensors are available via on-chip sensor
						fusion.
						<br>
						If we were not concerned about about keeping the wearable low-power,
						it would be an obvious choice to use the IC sensor fusion result to
						obtain arm orientation (angular displacement). However, this fusion
						core is responsible for around 35% of the power consumption of the
						module during "significant motion" (see Fig. 1).
						<br>
						<img src="Team/progress/zghera/img/week2/bhi160b_table_13_power_consumption.jpg" style="width:40%;height:40%;">
						<br>Fig. 1: Table 13 from BHI160B datasheet
						<br>
						Thus, the ability to disable the fusion core and perform this
						computation on the host would save up to ~1 mA which would translate
						to ~1.8mW. For reference, the analog event detector consumes 0.7mW.
						<br>
						Based on section 9.4 in the BHI160B datasheet, it appears you can
						disable the fusion core so that it will only draw ~7uA.
					</li>
					<li>
						<b>Functional Specification</b> 
						<br>
						I completed the functional description, expected use case,
						computational constraints, and functional block diagram sections
						of the report.
						<br>
						The RevEx functional block diagram I created is shown in Fig. 2.
						<br>
						<img src="Team/progress/zghera/img/week2/revex-fbd.jpg" style="width:75%;height:75%;">
						<br>Fig. 2: RevEx Functional Block Diagram
					</li>
				</ul>
			</p>
			<br>

			<h4>Week 3</h4>
			<p>
				<b>Date:</b> 9/10<br>
				<b>Total hours:</b> ~12 hours<br>
				<b>Description of design efforts:</b><br>
				<ul>
					<li>
						<b>Sensor Fusion Research</b> 
						<br>
						This week I spent much of my time researching methods to estimate 
						the users arm position from the following sensor/input sources:
						<ul>
							<li>Analog potentiometer voltage reading.</li>
							<li>3-axis gyroscope reading.</li>
							<li>3-axis accelerometer reading.</li>
							<li>VR headset position.</li>
							<li>(Configuration only) VR hand controller positions.</li>
						</ul>
						<br>
						This problem contains three sub-problems:
						<ol>
							<li>Estimate the position of the forearm relative to the upper arm.</li>
							<li>Estimate the position of the upper arm relative to the shoulder.</li>
							<li>Estimate the position of the shoulder relative to the VR headset.</li>
						</ol>
						I will discuss the proposed solutions for each below:
						<br>
						<br>
						Estimation #1 can be obtained using the position of the elbow (from
						estimation #2) and the voltage reading from the potentiometer.
						<br>
						<br>
						Skipping to #3, we can initially assume the shoulder position and
						then correct this guess during a calibration phase where the user
						is instructed to manuever the controllers in a specific way.
						<br>
						<br>
						Estimation #2 is the least trivial as the shoulder joint has 3 degrees
						of freedom. The challenge is to use the location of the shoulder
						and the IMU (located on the users upper arm) to estimate the position
						of the arm relative to the shoulder.
						<br>
						As mentioned above, our IMU does contain a sensor fusion core.
						However, in an effort to keep the system as low power as possible,
						the team decided to move the sensor fusion to the host computer.
						Thus, the primary objective was to find a sensor fusion method
						that has both high accuracy/robust and computationally inexpensive.
						<br>
						In my research so far, I have identified 2 alternative solutions:
						<ol>
							<li>
								Estimate arm attitude relative to shoulder.
								<br>
 								This would likely encompass fusing the gravitational
								vector estimate using the accelerometer (with some trigonometry)
								as well as the integrated rotational velocity from gyroscope.
								Based on initial research, a Kalman or complementary filter are 
								common choices for fusing this information. After researching the
								differences between the filters (e.g. 
								<a href="https://www.researchgate.net/publication/320726643_Comparison_of_Complementary_and_Kalman_Filter_Based_Data_Fusion_for_Attitude_Heading_Reference_System">
									Comparison of Complementary and Kalman Filter Based Data Fusion for Attitude Heading Reference System
								</a>
								), the complementary filter is the choice for attitude estimation
								in our application due to its simplicity, low computational cost,
								and comparable (although consistently worse) performance
								to the Kalman filter .
								<br>
								However, there are two issues with this approach.
								<ol>
									<li>
										Correcting "yaw" drift.
										<br>
										Without a magnetometer, there is no easy way to correct
										for estimated yaw angle as there is no gravity vector
										component we can extract from the accelerometer in that
										plane. The only counter to this point I found was in
										<a href="https://ieeexplore.ieee.org/document/6385893">
										this paper
										</a>
										. They suggest (section 3B) that we may be able to get
										around this if we assume that we don't spend a lot of
										time moving solely in this plane and then we can calculate
										the drift once the joint axis is no longer in the yaw plane 
										by "integrating the gyroscope readings and comparing
										against the joint angle." But I am not sure of how well
									  this applies for our use case.
									</li>
									<li>
										Linear acceleration.
										<br>
										 In all the papers/docs I read (e.g. see end of section 2 in
										 <a href="https://www.nxp.com/docs/en/application-note/AN3461.pdf"></a>
										 this document
										 ), they all ensure "smooth movements" or just flat out
										 assume no linear acceleration. This is not true for us.
										 And it makes sense why this is a problem. How would you
										 know the difference between linear acceleration and a
										 change in the gravitational vector components (unless
										 the magnitude is greater than ~1g of course). In this case,
										 we may not be able to reliably use the accelerometer to
										 correct for gyroscope drift at all times.
									</li>
								</ol>
							</li>
							<li>
								Estimate arm pose relative to shoulder.
								<br>
								While similar to the previous point, this involves estimating 
								the absolute IMU position using the accelerometer and gyroscope.
								The advantage here is reducing the need for a magnetometer and
								being able to more robustly handle linear acceleration of the 
								arm.
								<br>
								The main downside here is that we would need a more complex
								fusion algorithm such as the
								<a href="chrome-extension://cbnaodkpfinfiipjblikofhlhlcickei/src/pdfviewer/web/viewer.html?file=file:///C:/Users/zghera/Downloads/sensors-17-01257.pdf">
								EKF
								</a>
								. As a result, I would need to
								perform much more research on how to implement something like an
								EKF properly for our use case.
							</li>
						</ol>
						<br>
						After analysis of the alternatives, I determined that implementing
						and tuning our own sensor fusion algorithm would be time consuming 
						and was unnecessary as the sensor fusion core on our IMU was able to
						perform sensor fusion with a lower power cost associated.
						<br>
						<br>
						However, after doing some research on compatable magnetometer with
						the BHI160B, the only available magnetometers were very small 
						(1.5 x 1.5 mm) BGA chips that would be impossible to solder with 
						our current equipment.
						<br>
						However, I still believed that using an existing implementation to
						the sensor fusion (whether that be in software or hardware) made
						more sense for our project given the limited time and resources.
						<br>
						I helped search for a new IMU that had a built in magnetometer. We found the 
						<a href="https://www.sparkfun.com/products/15335">
						ICM-20948
						</a>
						It met all of our criteria (e.g. low power, >100 Hz update rate,
						easy to solder, etc.). 
						<br>
						<br>
						There were two options for sensor fusion with this IMU:
						<ol>
							<li>
								A built in FPGA for digital motion processing (DMP). Swag found an 
								<a href="https://github.com/ZaneL/Teensy-ICM-20948">
								Arduino library on GitHub 
								</a>
								that uses the IMU company's (Invensense) firmware for interfacing with the 
								DMP FPGA.
							</li>
							<li>
								An open source 9DOF IMU sensor fusion implementation such as
								<a href="https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/">
									x-io technologies IMU algorithms
								</a>
								.
							</li>
						</ol>
						After some poking around in both of the code repositories and
						doing some additonal research, I determined that the easiest path
						would be option 2. According to
						<a href="https://github.com/pimoroni/icm20948-python/issues/1#issuecomment-539730082">
						this GitHub issue comment 
						</a>
						, it was a challenge to interface with Invensense's FPGA. For example,
						you must register and get permission from Invensese to down load the
						proprietary binary image before you can use the FPGA. Additionally, performing
						the sensor fusion on host allows for easier modification and debugging.
						<br>
					</li>
					<li>
						<b>Software Overview</b> 
						<br>
						My other main task this week was to complete the software overview 
						document. I worked with Isaac and to determine some details such as the
						bluetooth communication and the microcontroller states. Outside of this 
						collaboration, I was responsible for writing the software overview 
						and working	through all the software design decisions encapsulated
						in the document.
						<br>
						The RevEx host computer flowchart I created is shown in Fig. 3.
						<br>
						<img src="Team/progress/zghera/img/week3/revex-flowchart-host-computer.jpg" style="width:85%;height:85%;">
						<br>Fig. 3: Host computer flowchart
					</li>
				</ul>
			</p>		
			<br>

			<h4>Week 4</h4>
			<p>
				<b>Date:</b> 9/17<br>
				<b>Total hours:</b> ~4 hours<br>
				<b>Description of design efforts:</b><br>
				I was unable to spend much time on senior design this week as I had
				big asssignments due in three of my other classes.
				<br>
				My time was split between the three tasks listed below:
				<ul>
					<li>
						<b>Digitization of hand-drawn signal flow diagrams for Electrical Overview.</b>
						<br>
						I transfered Swagat's initial hand-drawn sigal flow diagrams for the top-level system
						as well as the data aquisition and haptic feedback subsystems.
						The signal flow diagram for the haptic feedback system is shown in Fig. 4 below.
						<img src="Team/progress/zghera/img/week4/HMmS-Signal-Flow-Diagram.jpg" style="width:85%;height:85%;">
						<br>Fig. 4: Haptic feedback signal flow diagram
					</li>
					<li>
						<b>Assist with Bill of Materials</b>
						<br>
						I filled out the details for around 1/3 of our components in the Bill of Materials
						assignment.
					</li>
					<li>
						<b>Testing Host Computer Bluetooth Integration with Unreal Engine</b>
						The next step for the host computer software is to determine the best method 
						for bluetooth communication in conjuction with the rest of the VR system.
						We have yet to decide whether we will use Unreal or Unity as our VR engine.
						Matthew has a lot of experience working with Unity, so I thought that it would
						be good for me to get some exposure to Unreal Engine before we make our decision.
						<br>
						I went through the initial VR development tutorial for Unreal and did some research
						on the common practices for integrating bluetooth into a VR desktop application. One
						of the most popular methods is to create a seperate process using
						<a href="https://github.com/pimoroni/icm20948-python/issues/1#issuecomment-539730082">
							Windows Socket bluetooth library
						</a>
						that the VR simulation can subscribe to. I was able to create a sample class that
						initializes the Windows Socket DLL and run the initial VR sandbox scene (see Fig. 5).
						<img src="Team/progress/zghera/img/week4/unreal-example.jpg" style="width:100%;height:100%;">
						Fig 5: Unreal Engine VR Sandbox Scene with Successful Bluetooth Initialization
					</li>
				</ul>
			</p>


			<br>
		</div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
