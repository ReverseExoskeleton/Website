<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<base href="https://engineering.purdue.edu/477grp6/" />

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
			<h2>Progress Report for Zach Ghera</h2>
			
			<h4>Week 2</h4>
			<p>
				<b>Date:</b> 9/3<br>
				<b>Total hours:</b> ~8 hours<br>
				<b>Description of design efforts:</b><br>
				I spent the majority of this week doing research on the feasibility of
				hardware/modules that we need for our system. Please see a summary of each
				progress item below:
				<ul>
					<li>
						<b>Determined the hardware necessary for wireless communication.</b>
						<br>
						The first step I took was researching BLE versus alternatives
						such as RF (specifically ZigBee) to see which would be a more
						viable for our project. More specifically, the choosen wireless
						communication method needs to have:
						<ul>
							<li>Low power consumption and (ideally) 1.8V source voltage</li>
							<li>Low latency</li>
							<li>Low cost</li>
						</ul>
						In addition to looking at some of the most popular (available)
						chips for both 
						<a href="https://www.digikey.jp/products/en/rf-if-and-rfid/rf-transceiver-modules-and-modems/872?k=&pkeyword=xbee&sv=0&pv1883=320634&sf=0&FV=1989%7C0%2C-8%7C872&quantity=&ColumnSort=0&page=1&stock=1&pageSize=25">
							BLE 
						</a>
						and 
						<a href="https://www.digikey.jp/products/en/rf-if-and-rfid/rf-transceiver-modules-and-modems/872?FV=1989%7C0%2C-8%7C872%2C578%7C423589&quantity=0&ColumnSort=0&page=1&stock=1&pageSize=25&pkeyword=xbee">
							RF/Zigbee
						</a>
						, one valuable source to help inform the decision was 
						<a href="https://www.researchgate.net/publication/271327094_On_the_Potential_of_Bluetooth_Low_Energy_Technology_in_Vehicular_Applications">
							this paper on the Potential of Bluetooth Low Energy
							Technology in Vehicular Applications
						</a>
						. As summarized in the Comparison between BLE and ZigBee section,
						BLE offers a faster data rate, lower latency, lower cost, and
						more robust in the presence of strong WiFi interference.
						Therefore, I recommended that we proceed with buying a BLE chip.
						<br>
						I then did some research to get a sense of what are some of the
						most popular BLE controllers. 
						<a href="https://developex.com/blog/most-popular-ble-controllers/">
							This blog
						</a>
						was one of many websites that I referenced alongside filtering
						through different bluetooth chips on the digikey website. After
						some review of the available parts, I chose the 
						<a href="https://www.digikey.jp/product-detail/en/jorjin-technologies-inc/ZB7412-00/2393-ZB7412-00CT-ND/12142962">
						ZB7412 module 
						</a>
						based on the TI CC26xx BLE controller. The main primary rationale
						behind this choice was that it had the second lowest current
						consumption (behind the DA 14580 which had no available BLE modules).

					</li>
					<li>
						<b>Confirmed ability to disable IMU (
						<a href="https://www.digikey.com/en/products/detail/bosch-sensortec/BHI160B/9674245?s=N4IgTCBcDaIBxjgWgIwAY4E5VIHIBEQBdAXyA">
							BHI160B
						</a>
						) fusion core.</b>
						<br>
						After the team ordered a few IMUs, I found that this particular IMU
						had an onboard "fusion core" that allows a client to read from
						"virtual sensors." These sensors are available via on-chip sensor
						fusion.
						<br>
						If we were not concerned about about keeping the wearable low-power,
						it would be an obvious choice to use the IC sensor fusion result to
						obtain arm orientation (angular displacement). However, this fusion
						core is responsible for around 35% of the power consumption of the
						module during "significant motion" (see Fig. 1).
						<br>
						<img src="Team/progress/zghera/img/week2/bhi160b_table_13_power_consumption.jpg" style="width:40%;height:40%;">
						<br>Fig. 1: Table 13 from BHI160B datasheet
						<br>
						Thus, the ability to disable the fusion core and perform this
						computation on the host would save up to ~1 mA which would translate
						to ~1.8mW. For reference, the analog event detector consumes 0.7mW.
						<br>
						Based on section 9.4 in the BHI160B datasheet, it appears you can
						disable the fusion core so that it will only draw ~7uA.
					</li>
					<li>
						<b>Functional Specification</b> 
						<br>
						I completed the functional description, expected use case,
						computational constraints, and functional block diagram sections
						of the report.
						<br>
						The RevEx functional block diagram I created is shown in Fig. 2.
						<br>
						<img src="Team/progress/zghera/img/week2/revex-fbd.jpg" style="width:75%;height:75%;">
						<br>Fig. 2: RevEx Functional Block Diagram
					</li>
				</ul>
			</p>
			<br>

			<h4>Week 3</h4>
			<p>
				<b>Date:</b> 9/10<br>
				<b>Total hours:</b> ~12 hours<br>
				<b>Description of design efforts:</b><br>
				<ul>
					<li>
						<b>Sensor Fusion Research</b> 
						<br>
						This week I spent much of my time researching methods to estimate 
						the users arm position from the following sensor/input sources:
						<ul>
							<li>Analog potentiometer voltage reading.</li>
							<li>3-axis gyroscope reading.</li>
							<li>3-axis accelerometer reading.</li>
							<li>VR headset position.</li>
							<li>(Configuration only) VR hand controller positions.</li>
						</ul>
						<br>
						This problem contains three sub-problems:
						<ol>
							<li>Estimate the position of the forearm relative to the upper arm.</li>
							<li>Estimate the position of the upper arm relative to the shoulder.</li>
							<li>Estimate the position of the shoulder relative to the VR headset.</li>
						</ol>
						I will discuss the proposed solutions for each below:
						<br>
						<br>
						Estimation #1 can be obtained using the position of the elbow (from
						estimation #2) and the voltage reading from the potentiometer.
						<br>
						<br>
						Skipping to #3, we can initially assume the shoulder position and
						then correct this guess during a calibration phase where the user
						is instructed to manuever the controllers in a specific way.
						<br>
						<br>
						Estimation #2 is the least trivial as the shoulder joint has 3 degrees
						of freedom. The challenge is to use the location of the shoulder
						and the IMU (located on the users upper arm) to estimate the position
						of the arm relative to the shoulder.
						<br>
						As mentioned above, our IMU does contain a sensor fusion core.
						However, in an effort to keep the system as low power as possible,
						the team decided to move the sensor fusion to the host computer.
						Thus, the primary objective was to find a sensor fusion method
						that has both high accuracy/robust and computationally inexpensive.
						<br>
						In my research so far, I have identified 2 alternative solutions:
						<ol>
							<li>
								Estimate arm attitude relative to shoulder.
								<br>
 								This would likely encompass fusing the gravitational
								vector estimate using the accelerometer (with some trigonometry)
								as well as the integrated rotational velocity from gyroscope.
								Based on initial research, a Kalman or complementary filter are 
								common choices for fusing this information. After researching the
								differences between the filters (e.g. 
								<a href="https://www.researchgate.net/publication/320726643_Comparison_of_Complementary_and_Kalman_Filter_Based_Data_Fusion_for_Attitude_Heading_Reference_System">
									Comparison of Complementary and Kalman Filter Based Data Fusion for Attitude Heading Reference System
								</a>
								), the complementary filter is the choice for attitude estimation
								in our application due to its simplicity, low computational cost,
								and comparable (although consistently worse) performance
								to the Kalman filter .
								<br>
								However, there are two issues with this approach.
								<ol>
									<li>
										Correcting "yaw" drift.
										<br>
										Without a magnetometer, there is no easy way to correct
										for estimated yaw angle as there is no gravity vector
										component we can extract from the accelerometer in that
										plane. The only counter to this point I found was in
										<a href="https://ieeexplore.ieee.org/document/6385893">
										this paper
										</a>
										. They suggest (section 3B) that we may be able to get
										around this if we assume that we don't spend a lot of
										time moving solely in this plane and then we can calculate
										the drift once the joint axis is no longer in the yaw plane 
										by "integrating the gyroscope readings and comparing
										against the joint angle." But I am not sure of how well
									  this applies for our use case.
									</li>
									<li>
										Linear acceleration.
										<br>
										 In all the papers/docs I read (e.g. see end of section 2 in
										 <a href="https://www.nxp.com/docs/en/application-note/AN3461.pdf"></a>
										 this document
										 ), they all ensure "smooth movements" or just flat out
										 assume no linear acceleration. This is not true for us.
										 And it makes sense why this is a problem. How would you
										 know the difference between linear acceleration and a
										 change in the gravitational vector components (unless
										 the magnitude is greater than ~1g of course). In this case,
										 we may not be able to reliably use the accelerometer to
										 correct for gyroscope drift at all times.
									</li>
								</ol>
							</li>
							<li>
								Estimate arm pose relative to shoulder.
								<br>
								While similar to the previous point, this involves estimating 
								the absolute IMU position using the accelerometer and gyroscope.
								The advantage here is reducing the need for a magnetometer and
								being able to more robustly handle linear acceleration of the 
								arm.
								<br>
								The main downside here is that we would need a more complex
								fusion algorithm such as the
								<a href="chrome-extension://cbnaodkpfinfiipjblikofhlhlcickei/src/pdfviewer/web/viewer.html?file=file:///C:/Users/zghera/Downloads/sensors-17-01257.pdf">
								EKF
								</a>
								. As a result, I would need to
								perform much more research on how to implement something like an
								EKF properly for our use case.
							</li>
						</ol>
						<br>
						After analysis of the alternatives, I determined that implementing
						and tuning our own sensor fusion algorithm would be time consuming 
						and was unnecessary as the sensor fusion core on our IMU was able to
						perform sensor fusion with a lower power cost associated.
						<br>
						<br>
						However, after doing some research on compatable magnetometer with
						the BHI160B, the only available magnetometers were very small 
						(1.5 x 1.5 mm) BGA chips that would be impossible to solder with 
						our current equipment.
						<br>
						However, I still believed that using an existing implementation to
						the sensor fusion (whether that be in software or hardware) made
						more sense for our project given the limited time and resources.
						<br>
						I helped search for a new IMU that had a built in magnetometer. We found the 
						<a href="https://www.sparkfun.com/products/15335">
						ICM-20948
						</a>
						It met all of our criteria (e.g. low power, >100 Hz update rate,
						easy to solder, etc.). 
						<br>
						<br>
						There were two options for sensor fusion with this IMU:
						<ol>
							<li>
								A built in FPGA for digital motion processing (DMP). Swag found an 
								<a href="https://github.com/ZaneL/Teensy-ICM-20948">
								Arduino library on GitHub 
								</a>
								that uses the IMU company's (Invensense) firmware for interfacing with the 
								DMP FPGA.
							</li>
							<li>
								An open source 9DOF IMU sensor fusion implementation such as
								<a href="https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/">
									x-io technologies IMU algorithms
								</a>
								.
							</li>
						</ol>
						After some poking around in both of the code repositories and
						doing some additonal research, I determined that the easiest path
						would be option 2. According to
						<a href="https://github.com/pimoroni/icm20948-python/issues/1#issuecomment-539730082">
						this GitHub issue comment 
						</a>
						, it was a challenge to interface with Invensense's FPGA. For example,
						you must register and get permission from Invensese to down load the
						proprietary binary image before you can use the FPGA. Additionally, performing
						the sensor fusion on host allows for easier modification and debugging.
						<br>
					</li>
					<li>
						<b>Software Overview</b> 
						<br>
						My other main task this week was to complete the software overview 
						document. I worked with Isaac and to determine some details such as the
						bluetooth communication and the microcontroller states. Outside of this 
						collaboration, I was responsible for writing the software overview 
						and working	through all the software design decisions encapsulated
						in the document.
						<br>
						The RevEx host computer flowchart I created is shown in Fig. 3.
						<br>
						<img src="Team/progress/zghera/img/week3/revex-flowchart-host-computer.jpg" style="width:85%;height:85%;">
						<br>Fig. 3: Host computer flowchart
					</li>
				</ul>
			</p>		
			<br>

			<h4>Week 4</h4>
			<p>
				<b>Date:</b> 9/17<br>
				<b>Total hours:</b> ~4 hours<br>
				<b>Description of design efforts:</b><br>
				I was unable to spend much time on senior design this week as I had
				big asssignments due in three of my other classes.
				<br>
				My time was split between the three tasks listed below:
				<ul>
					<li>
						<b>Digitization of hand-drawn signal flow diagrams for Electrical Overview.</b>
						<br>
						I transfered Swagat's initial hand-drawn sigal flow diagrams for the top-level system
						as well as the data aquisition and haptic feedback subsystems.
						The signal flow diagram for the haptic feedback system is shown in Fig. 4 below.
						<img src="Team/progress/zghera/img/week4/HMmS-Signal-Flow-Diagram.jpg" style="width:85%;height:85%;">
						<br>Fig. 4: Haptic feedback signal flow diagram
					</li>
					<li>
						<b>Assist with Bill of Materials</b>
						<br>
						I filled out the details for around 1/3 of our components in the Bill of Materials
						assignment.
					</li>
					<li>
						<b>Testing Host Computer Bluetooth Integration with Unreal Engine</b>
						The next step for the host computer software is to determine the best method 
						for bluetooth communication in conjuction with the rest of the VR system.
						We have yet to decide whether we will use Unreal or Unity as our VR engine.
						Matthew has a lot of experience working with Unity, so I thought that it would
						be good for me to get some exposure to Unreal Engine before we make our decision.
						<br>
						I went through the initial VR development tutorial for Unreal and did some research
						on the common practices for integrating bluetooth into a VR desktop application. One
						of the most popular methods is to create a seperate process using
						<a href="https://github.com/pimoroni/icm20948-python/issues/1#issuecomment-539730082">
							Windows Socket bluetooth library
						</a>
						that the VR simulation can subscribe to. I was able to create a sample class that
						initializes the Windows Socket DLL and run the initial VR sandbox scene (see Fig. 5).
						<img src="Team/progress/zghera/img/week4/unreal-example.jpg" style="width:100%;height:100%;">
						Fig 5: Unreal Engine VR Sandbox Scene with Successful Bluetooth Initialization
					</li>
				</ul>
			</p>
			<br>

			<h4>Week 5</h4>
			<p>
				<b>Date:</b> 9/24<br>
				<b>Total hours:</b> ~8 hours<br>
				<b>Description of design efforts:</b><br>
				This week, my efforts focused on the host-side BLE interface and the VR
				simulation.
				<br>
				After my experience working in Unreal last week, I finally got a taste 
				of how little I know about game development and game engines. That being
				said, I talked with Mattew and decided that choosing Unity as our game
				engine would make much more sense as Matt has 3+ years of experience developing
				VR applications in Unity. Thus, developing in Unity	would speed up my
				learning process a lot as Matt can answer any questions I have. 
				<br>
				Having decided to use Unity, I did some research to determine the simplest
				way to integrate BLE communication into a Unity application. The main problem
				is that the Unity editor cannot access the .Net-Core library needed for 
				the Windows BLE library (UWP API) (see this
				<a href="https://stackoverflow.com/questions/60686302/ble-device-with-unity/65403179">
					Stack Overflow Post
				</a>
				and this
				<a href="https://mtaulty.com/2019/03/22/rough-notes-on-experiments-with-uwp-apis-in-the-unity-editor-with-c-winrt/">
					blog post
				</a>
				for more information). The typical solution is to call the UWP API from a C++
				application and then create a dll that components in Unity can use.
				<br>
			  Due to the popularity of using BLE in Unity projects, people have released
				open source code to interact with the UWP BLE API. I read the code and 
				tested a few of the open source implementations and found one that will
				suite our needs well: 
				<a href="https://github.com/adabru/BleWinrtDll">Adam Brunnmeier's BleWinrtDll</a>
				. See Fig. 6 for a screenshot of the BleWinrtDll sample demo I tested in
				Unity from within my appartment.
				<br>
				<img src="Team/progress/zghera/img/week5/unity-ble-demo.jpg" style="width:75%;height:75%;">
				<br>Fig. 6: Unity BleWinrtDll Demo
				<br>
				<br>
				Now that we have a easy way to connect and read data over BLE, the next step
				for me was to get some fundamentals in Unity development. At the same time, 
				testing the sensor fusion algorithm identified in week 3 (see above) is a
				critical step. However Isaac is still working on communication between 
				the MCU and our BLE chip. So we decided that in the meantime it would be
				beneficial to send the raw IMU data over serial from an Arduino to the
				host-computer. 
				<br>
				To accomplish both goals, I started developing a Unity scene that reads 
				data from a serial port and displays the raw IMU data in a 2D dashboard (
				see Fig. 7). As part of this, I am reading/watching a lot of basic 
				Unity tutorials and asking Matthew a lot of questions as I develop this 
				initial prototype scene and try to get up to speed with Unity.
				<br>
				<img src="Team/progress/zghera/img/week5/unity-imu-data-display.jpg" style="width:95%;height:95%;">
				<br>Fig. 7: Unity IMU Data Serial Display
			</p>
			<br>

			<h4>Week 6</h4>
			<p>
				<b>Date:</b> 10/1<br>
				<b>Total hours:</b> ~12 hours<br>
				<b>Description of design efforts:</b><br>
				This week was soely focused on development and testing of the serial
				communication interface transmitting the IMU data from the Arduino
				to the host computer. Although we will be using the STM32 and BLE for
				transmitting sensor data, Isaac is starting to realize that interfacing
				with the BLE and IMU chips is non-trivial. Thus, the importance of this
				serial communication is critical to facilitate development of the software
				on the host computer.
				<br>
				I need live sensor data to test the sensor fusion and other aspects of the
				VR simulation while Isaac works on the interfacing with the IMU and BLE with
				the STM MCU. Once Isaac is ready for BLE, all I have to do is swap in the
				serial tranceiver object with an analogous BLE tranceiver object. Additionally,
				if there are ever any problems with the STM MCU once BLE and IMU interfacing is
				complete, I can still continue testing the host computer with the serial interface.
				<br><br>
				Isaac found an 
				<a href="https://github.com/sparkfun/SparkFun_ICM-20948_ArduinoLibrary">
				Arduino library for the IMU chip
				</a>
				and used the 
				<a href="https://github.com/sparkfun/SparkFun_ICM-20948_ArduinoLibrary/blob/main/examples/Arduino/Example5_DualSPITest/Example5_DualSPITest.ino">
				dual SPI test script
				</a>
				as a starting point. After Isaac modified the script such that we only used a single SPI
				line with our IMU, I wrote some Arduino code to write IMU packets (based on the structure
				defined in the software overview) over the serial line (See Fig. 8).
				<br>
				As of now, `floatToBytes()` simply `memcpy`s the 16 bit float into the byte buffer. However,
				depending on architecutre of the chip the Arduino uses, I may need to do some additional
				manipulation so that the host can properly decode the two bytes into a 16-bit float.
				<br><br>
				<img src="Team/progress/zghera/img/week6/arduino-imu-serial.jpg" style="width:50%;height:50%;">
				<br>Fig. 8: Arduino Script to Send IMU Packer over Serial Line
				<br>
				<br>
				I also wrote the serial interface on the host-side from using the Windows SerialPort
				library and C# threads (see Fig. 9 and Fig. 10). 

				<div style="display: flex;align-items: flex-start;">
					<div style="flex: 50%;padding: 5px;">
						<img src="Team/progress/zghera/img/week6/host-imu-serial1.jpg" style="width:100%;height:100%;">
						<br>Fig. 9: SerialReader Class that is Responsible for Reading and Decoding IMU Packets Received on the Host Computer.
					</div>
					<div style="flex: 50%;">
						<img src="Team/progress/zghera/img/week6/host-imu-serial2.jpg" style="width:100%;height:100%;">
						<br>Fig. 10: Other Classes Used by the SerialReader Class on the Host Computer.
					</div>
				</div>
				<br><br>

				I has some initial success with this implementation this week. As seen in Fig. 11, the IMU
				data is successfully received on the host computer. While this is a great first step, there
				are multiple areas for improvement before I can move on to implementing and testing sensor fusion:
				<ol>
					<li><b>Thread reader vs. Packet received event handler</b>
						<br>
						There is a build in event handler method build into the SerialPort class using C# delegates.
						I initially tried this approach but the event was never triggered so it appeared that no 
						packets were received on the host. I want to see if I can get this method working as it is
						more readable, efficient, and less error prone than manually manipulating threads and
						packet retreival.
					</li>
					<li><b>Filter out IMU Start Message from Packet Retreival</b>
						<br>
						As seen in Fig. 11, the first thing the Arduino sends over serial is an initialization
						string from the IMU. I need to be able to filter this out on the host such that
						the the data packets are aligned correctly (i.e. don't let chars from init message sneak
						into the data packets).
					</li>
					<li><b>Verify Floating Point Byte Encode/Decode Strategy</b>
						<br>
						As mentioned above, verify if the float memcpy approach works for serializing the 
						16-bit floats provided by the IMU library. If not, find a different approach to
						encode the floats as two bytes and then decode back to floats on the host.
					</li>
					<li><b>Arduino packet send at 100 Hz</b>
						<br>
						Because we will be sending over bluetooth at 100 Hz, we should replicate this in the 
						serial send interface as well.
					</li>
					<li><b>Update UI in Unity</b>
						<br>
						As seen in Fig. 7 of week 5, I created a toy UI. The final step once all the communications
						are working properly is to update this UI to help get me more familiar with interacting with
						Unity game objects and have a prettier way to view current IMU data before implementing
						the sensor fusion and associated scene.
					</li>
				</ol>

				<br><br>
				<img src="Team/progress/zghera/img/week6/imu-data-received-in-unity.jpg" style="width:45%;height:45%;">
				<br>Fig. 11: Unity Terminal Output When Running Both the Arduino and Host-Computer IMU 
				Packet Transmit Code.
				<br><br>
			</p>
			<br>

			<h4>Week 7</h4>
			<p>
				<b>Date:</b> 10/8<br>
				<b>Total hours:</b> ~14 hours<br>
				<b>Description of design efforts:</b><br>
				This week was focused on continuing the efforts for transmitting and
				displaying IMU data on the host computer. I was able to get a fully
				function test working this week (see Fig. 12).
				<br><br>
				<img src="Team/progress/zghera/img/week7/unity-imu-data-panel.jpg" style="width:65%;height:65%;">
				<br>Fig. 12: Simple Unity IMU Data Panel.
				<br><br>
				Please see a quick summary of my soltions for each improvement areas that I listed in week 6:
				<ol>
					<li><b>Thread reader vs. Packet received event handler</b>
						<br>
						I found that this DataReceived event does not work unless you send specific characters - see
						<a href="https://www.vgies.com/a-reliable-serial-port-in-c/">this article</a>
						. Instead, I took the advice of the article and used the BaseStream object of SerialPort
						to accomplish asynchronous reading from the input buffer.
					</li>
					<li><b>Filter out IMU Start Message from Packet Retreival</b>
						<br>
						This was accomplished with simple string subscripting and loops. 
					</li>
					<li><b>Verify Floating Point Byte Encode/Decode Strategy</b>
						<br>
						The memcpy strategy did not product correct results. Instead, I adapted 
						<a href="https://stackoverflow.com/a/37761168">
							an implementation of the IEEE 754-2008 half-precision float conversion
						</a>
						for encoding and decoding the 16-bit float on the MCU and host, respectively.
					</li>
					<li><b>Arduino packet send at 100 Hz</b>
						<br>
						I accomplished this by changing the baud rate of the packet send and Unity's
						preffered frame update rate.
					</li>
					<li><b>Update UI in Unity</b>
						<br>
						I watched some additional Unity tutorials on connecting scripts to game objects to figure
						out how to properly update the text elements of the static UI I created in week 5.
					</li>
				</ol>
				<br>

				I also spent a significant portion of my week helping Isaac with the software formalization 
				this week. Here is a summary of my contributions:
				<ul>
					<li>Section 1: Table entries for x-io technologies Fusion, Unity, and BleWinrtDll.</li>
					<li>
						Section 2: Host Computer VR Simulation (and sub-sub-sections) subsection and the 
						host portion of the BLE section.
					</li>
					<li>Section 3: Host Computer VR Simulation subsection.</li>
					<li>Component Diagrams: Top-level and host computer specific (see Fig. 13) diagrams.</li>
				</ul>
				<br><br>
				<img src="Team/progress/zghera/img/week7/software-formalization-host.jpg" style="width:65%;height:65%;">
				<br>Fig. 13: Software Formalization Host Software Component/Progress Diagram.
				<br><br>
			</p>
			<br>

			<h4>Week 9</h4>
			<p>
				<b>Date:</b> 10/22<br>
				<b>Total hours:</b> ~15 hours<br>
				<b>Description of design efforts:</b><br>
				This week was focused on getting the upper arm orientation vector using
				the Madgwick filter. 
				<br>
				I was able to get the Madgwick filter working for rotations in the roll
				and pitch axis (z and x in Unity) but am having problems with getting the
				yaw rotation (See Fig. 14 and Fig. 15).
				<br><br>
				<iframe width="500" height="400"
					src="https://www.youtube.com/embed/D6xg75PiZW0">
				</iframe>
				<br>Fig. 14: Unity Madgwick Prototyping Screenshare
				<br><br>
				<iframe width="500" height="400"
					src="https://www.youtube.com/embed/BodKs3Jfh3A">
				</iframe>
				<br>Fig. 15: Unity Madgwick Prototyping with IMU in Recording
				<br><br>
				The high-level steps I took to implement the Madwick filter demo in Unity
				are:
				<ol>
					<li>Create a DLL for the 
						<a href="https://github.com/xioTechnologies/Fusion">
							official Madgwick implementation</a>.
					</li>
					<li>Consuming the native Madgwick and creating an API in C#.</li>
					<li>Porting the
						<a href="https://github.com/xioTechnologies/Fusion/blob/master/Examples/ExampleAhrs.c">
							9-DOF AHRS example
						</a>
						to C#.</li>
					<li>Create a basic unity scene to visualize the Quaternion output.</li>
				</ol>
				Each of these steps required a good amount of learning as I have not
				worked with DLLs and have limited experience with Unity.
				<br>
				That being said, while I was partially successfull, I have yet to been
				able to figure out why yaw is not captured in the Quaternion output of
				the filter. Here are a few things I have done so far to debug:
				<ol>
					<li>Ensure that the IMU Arduino library does the propper
						calibration and scaling of the raw sensor values.
					</li>
					<li>Created an scene based on the Fusion library's 
						<a href="https://github.com/xioTechnologies/Fusion/blob/master/Examples/ExampleCompass.c">
							compass example</a>
						to confirm that it was not an error in the AHRS update function.
						<br>
						<i>I found the same results as I did with the AHRS example (i.e.
						the yaw angle did not change).</i> 
					</li>
					<li>Add logging to the initial implementation to see where the
						z axis of the Quaternion is not being updated upon rotation (e.g.
						after initialization, after integration, etc.).
						<br>
						<i>I did not see any obvious errors in these logs that would indicate a
						particular expression is causing an issue.</i>
					</li>
					<li> Replaced the Madgwick DLL C# API with the 
						<a href="https://github.com/xioTechnologies/Open-Source-AHRS-With-x-IMU">older C# implementation</a>
						from the same company (x-io technologies).
						<br>
						<i>This gave the same results as the C-based Fusion library.</i>
					</li>
					<li>Currently implementing the Madgwick impelementation entirely on 
						our Arduino Due to verify it is not an issue with the DLL creation
						or my C# API.
				</ol>
				For more details regarding the issues I am having and areas of potential
				confusion, please see the
				<a href="https://github.com/xioTechnologies/Fusion/issues/18">GitHub issue</a>
				that I opened on the official Madgwick implementation repo.
			</p>
			<br>


			<h4>Week 10</h4>
			<p>
				<b>Date:</b> 10/29<br>
				<b>Total hours:</b> ~15 hours<br>
				<b>Description of design efforts:</b><br>
				My efforts this week were focused on two software subsystems:
				sensor fusion and bluetooth communication.
				<br>
				<h5>Sensor Fusion</h5>
				I picked up where I left off last week: determining and fixing the sensor fusion
				algorithm yaw sensitivity.
				<br><br>
				Matthew graciously helped me run a few experiments to 
				help narrow down the source of the bug. More specifically, he tried running a
				C# implementation of the Mahony filter from the same authors as the
				official C implementation. There were a few problems with these implementations as
				there were dependencies on DLLs where they did not provide the original source. Additionally,
				the last time this code was updated was 8 years ago whereas the latest verison was updated
				3 years ago. Needless to say, these re-implementations had large issues with drift.
				Matthew also tried to re-implemented the Madgwick filter himself from scratch in C#.
				Unfortunately, this was also unsuccessful due as he did not have much time to dedicate to
				debugging.
				<br><br>
				I tried running the original C library on the Arduino to see if there was an issue with 
				corruption over communication, or more likely, a bug in my DLL. However, I was unable to
				flash the Arduino with my new code for some reason (I later figured out it was Because
				I forgot to re-install Invensense tools with my new install). So I moved back into
				my DLL C# API to try to fix the issue.
				<br><br>
				I do not remember all of the details that led up to the final realization. But at some
				point while experimenting with different changes to usage of the Madgwick library, I
				used the raw accelerometer and gyroscope values (received over serial) rather than calling
				the calibration functions in the library. This made the output sensitive to changes
				in yaw!!! I believe this was because these sensors were already calibrated by the
				arduino library we use and somehow the repeated calibration led to attenuating the z axis
				of the quaternion. Once I experimentally found the hard iron bias, I had something that 
				worked nearly as expected!!
				<br><br>
				There were two other major changes that I made to improve upon this discovery and result in
				the demo video in Fig. 16.:
				<ol>
					<li><b>Stationary yaw drift</b>
						<br>
						There was small drift in the yaw axis. I found that the library did have a gyro bias
						correction function. However, the threshold to be considered 'stationary' and the time
						before the correction algorithm activated was a long time (5 sec). After correcting these
						two parameters, the yaw drift was drastically minimized. However, there is still some
						stationary yaw drift. I plan to modify the algorithm to record the yaw angle once stationary
						to more aggressively correct any drift.
					</li>
					<li><b>IMU data tx / rx speeds</b>
						<br>
						I thought that I had corrected the Arduino to transmit at the same speed as the update
						rate of Unity. However, I had been doing testing on my monitor. When testing on my
						laptop screen, the tx and rx rates no longer matched up and the packet queue was 
						becoming full. My original solution to that was not great as I would throw away one packet
						at a time when it would become full. This led to a major lag between when I would move
						the IMU and when I would see the change on the screen. To fix this, Unity now consumes
						all of the packets waiting in the queue on each update to mitigate this issue. 
					</li>
				</ol>
				<br>
				<iframe width="500" height="400"
					src="https://www.youtube.com/embed/sWjhQOD6EGs">
				</iframe>
				<br>Fig. 16: Unity Madgwick Prototyping with IMU in Recording
				<br><br>
				<h5>Bluetooth Communication</h5>
				I worked with Isaac this week to get bluetooth communication working. Luckily, the BLE DLL
				I found on GitHub had a demo that worked very well once I worked with Isaac to debug
				GATT service creation issues on his side.
				<br>
				Once we resolved the bugs, I was able to connect to the Rev device and then subscribe to a
				specific GATT service where he send me the raw IMU data (See Fig. 17).
				<br><br>
				<img src="Team/progress/zghera/img/week10/ble-integration-demo.jpg" style="width:85%;height:85%;">
				<br>Fig. 17: Unity BLE Communication Demo.
				<br>
			</p>
			<br>

			<h4>Week 11</h4>
			<p>
				<b>Date:</b> 11/5<br>
				<b>Total hours:</b> ~4 hours<br>
				<b>Description of design efforts:</b><br>
				I was unable to make much progress this week due to an unexpected
				health emergency that incapacitated me for much of the week.
				<br><br>
				The time that I did spend on the project was focused on creating the
				Unity project that the final VR simulation will live. This meant
				refactoring my prototyping to enable easier debugging (e.g. Logger
				class), testing (e.g. Tranceiver interface to easily change based on
				whether we are using BLE vs serial to get data) and general extensibility
				/ maintainability.
				<br><br>
				One noteworthy change is for decoding the IMU data included in the sensor
				packets. Previously, we were sending 16-bit values that represented scaled
				floating points. This required 3rd party code for encoding and decoding on
				the MCU and host respectively. Instead of this, Isaac and I agreed that the
				MCU should instead send the raw 16-bit integer to the host where the host
				can simply case to a float and scale the values based on the configuration
				values for the IMU sensors (see Fig. 18).
				<br><br>
				<img src="Team/progress/zghera/img/week11/imu-sample-decode.jpg" style="width:85%;height:85%;">
				<br>Fig. 18: New IMU Sample Decoding Scheme (C# Code).
				<br>
			</p>
			<br>

			<h4>Week 12</h4>
			<p>
				<b>Date:</b> 11/12<br>
				<b>Total hours:</b> ~12 hours<br>
				<b>Description of design efforts:</b><br>
				This week I was able to spend a bit more time on senior design
				as I started to gain control over the medical flare up near the end of
				this week. My time was split around 60/40 between the environmental 
				and individial ethical analysis assignment and host computer software
				development, respectively.
				<br><br>
				The time I dedicated to host computer software development was focused
				on creating the BLE trainceiver module. As the name suggests, this module
				is responsible for connecting to, receiving, and transmitting data to
				the BLE module on the RevEx wearable. <br>
				This involved interfacing with the Windows BLE API using
				<a href="https://github.com/adabru/BleWinrtDll">Adam Brunnmeier's BleWinrtDll</a>
				. I was able to finish the components related to connection (to device
				and GATT service), subscribing to the GATT characteristic,
				and setting up the reader thread that constructs and adds incoming 
				packets to the read queue (See Fig. 19 for a code snippit).
				<br><br>
				The only peice left is transmitting packets from the host computer
				to the wearable. I plan to completing this before I return back to campus
				this weekend so I can test Bluetooth capabilities with Isaac this next week.
				<br><br>
				<img src="Team/progress/zghera/img/week12/ble-trainceiver1.jpg" style="width:60%;height:60%;">
				<img src="Team/progress/zghera/img/week12/ble-trainceiver2.jpg" style="width:60%;height:60%;">
				<img src="Team/progress/zghera/img/week12/ble-trainceiver3.jpg" style="width:60%;height:60%;">
				<img src="Team/progress/zghera/img/week12/ble-trainceiver4.jpg" style="width:60%;height:60%;">
				<br>Fig. 19: Ble Tranceiver Implementation (C# Code).
				<br>
			</p>
			<br>

			<h4>Week 13</h4>
			<p>
				<b>Date:</b> 11/12<br>
				<b>Total hours:</b> 3 + est. 6  hours<br>
				<b>Description of design efforts:</b><br>
				I was expecting to return back to campus early this week. However,
				per my doctors recommendation, I ended up staying home. However, I 
				have made plans to come back to campus this Sunday to do some bluetooth
				testing with Isaac.
				<br><br>
				In preparation for the Sunday meeting, I did two things to ensure that
				host-side software is bug free:
				<ol>
					<li><b>Finish BleTrainceiver</b><br>
						As noted last week, the only peice left to finish on the
						BleTrainceiver was the code for transmitting haptic packets
						back to the MCU. I finished that function this week (see Fig. 20).
					</li>
					<li><b>Test Madgwick Demo in New VR Unity Project</b><br>
						Although I was unable to return to campus this week, my parents were 
						able to grab my Arduino and IMU breakout setup when they went up to
						campus this past weekend. With hardware in hand, I spent a few hours
						testing and debugging the new unity project using the same Madgwick
						cube demo from previous weeks. To accomplish this, I wrote a new 
						function on the Arduino for sending the unscaled IMU values (see
						Fig. 21). In addition to handling the IMU value scaling, I also abstracted
						the sensor data that the host will receive from the MCU into a structure.
						With our current setup, this structure contains both the 3 Vector3s
						representing the calibrated IMU data as well as a 16-bit value that
						represents the elbow angle (See Fig. 22). The main bug that I found during
						testing was the accelerometer and gyroscope scaling values. After
						observing some odd behavior with the cube demo, I read through the
						Arduino library code a bit to find that I coppied down the wrong 
						scale values. After correcting this, everything looks good again!
					</li>
				</ol>
				<br><br>
				<img src="Team/progress/zghera/img/week13/send-haptic.jpg" style="width:60%;height:60%;">
				<br>Fig. 20: BleTrainceiver Haptic Feedback Send Function.<br><br>
				<img src="Team/progress/zghera/img/week13/send-unscaled.jpg" style="width:70%;height:70%;">
				<br>Fig. 21: Arduino Send Unscaled IMU Data Function.<br><br>
				<img src="Team/progress/zghera/img/week13/sensor-sample.jpg" style="width:60%;height:60%;">
				<br>Fig. 22: Sensor Sample Struct Constructor.<br><br>
				<img src="Team/progress/zghera/img/week13/sensor-scaling.jpg" style="width:60%;height:60%;">
				<br>Fig. 23: Correct IMU Scaling Constants.<br><br>
				<br>
				I plan to spend a few hours with Isaac on Sunday and then a few hours
				after our meeting to flesh out the shoulder joint and arm-length calibration
				methods needed to determine the arm joint locations.
			</p>
			<br>


			<h4>Week 14</h4>
			<p>
				<b>Date:</b> 11/28<br>
				<b>Total hours:</b> ~15 hours<br>
				<b>Description of design efforts:</b><br>
				My efforts this week were focused on two software subsystems:
				sensor fusion and bluetooth communication.
				<br>
				<h5>Sensor Fusion</h5>
				The remaining steps for sensor fusion include the arm joint calibration
				sequence and arm joint inference. I completed the implementation for both 
				of these components this week.
				<br>
				Arm joint calibration consists of determining the position of the shoulder
				joint and the length of the user's upper and forearm. The original strategy
				for doing this involved instructing the user to move their arm in an arc
				along a specific plane and sampling points along that arc. With multiple
				points, the perpendicular bisector could be determined for multiple points
				and the intersections of those lines would give us the shoulder and elbow
				positions. This strategy was somewhat complex and prone to user error.
				<br><br>
				I created a new strategy that involves the user placing their arms in
				two static poses (See Fig. 24 for an example) to determine the 
				shoulder position, and then the arm length. With this new strategy, 
				arm inference is as easy as updating 
				the transform rotations for the shoulder and elbow joints using the
				Madgwick filter output (transformed to shoulder coord frame) and 
				new elbow angle, respectively. See Fig. 25 for a short demo
				of the transform setup in Unity. For more details, please see
				the 
				<a href="https://docs.google.com/document/d/1x1XkzI_yQpbs2ew2pYxwAr0hBD2qGhd-EBrb9sx52KM/edit?usp=sharing">
					VR Simulation Planning document
				</a>
				.
				<br><br>
				Now that the code is written, I will start testing with Matthew's 
				VR hardware first thing this week. Once that is complete, the only
				outstanding host-software component that needs implemented is a VR scene
				that extracts force/torque data from physics interactions of the user
				and objects in VR. This component will be a completed in collaboration
				between Matthew and I.

				<br><br>
				<img src="Team/progress/zghera/img/week14/total-arm-length.jpg" style="width:50%;height:50%;">
				<br>Fig. 24: Diagram to Illustrate Arm Length Calculation.<br><br>
				
				<br>
				<iframe width="700" height="400"
					src="https://www.youtube.com/embed/qtU2VrsTMMc">
				</iframe>
				<br>Fig. 25: Unity Arm Estimation Transforms Setup
				<br><br>

				<h5>Bluetooth Communication</h5>
				I also worked a bit with Isaac this week to test the new BleTrainceiver
				with the MCU. After debugging some issues on the MCU side, we were able
				to bi-directionally send data. I was also able to properly decode IMU
				data sent from the MCU to the host as expected. 
				<br>
				There are currently some bugs on the MCU side with corrupted IMU values.
				Once we resolve these, we should be able to replicate the Madgwick demo 
				using BLE rather than serial.
			</p>
			<br>

			<br>
		</div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
