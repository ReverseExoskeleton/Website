<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<base href="https://engineering.purdue.edu/477grp6/" />

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
			<h2>Progress Report for Zach Ghera</h2>
			
			<h4>Week 2</h4>
			<p>
				<b>Date:</b> 9/3<br>
				<b>Total hours:</b> ~8 hours<br>
				<b>Description of design efforts:</b><br>
				I spent the majority of this week doing research on the feasibility of
				hardware/modules that we need for our system. Please see a summary of each
				progress item below:
				<ul>
					<li>
						<b>Determined the hardware necessary for wireless communication.</b>
						<br>
						The first step I took was researching BLE versus alternatives
						such as RF (specifically ZigBee) to see which would be a more
						viable for our project. More specifically, the choosen wireless
						communication method needs to have:
						<ul>
							<li>Low power consumption and (ideally) 1.8V source voltage</li>
							<li>Low latency</li>
							<li>Low cost</li>
						</ul>
						In addition to looking at some of the most popular (available)
						chips for both 
						<a href="https://www.digikey.jp/products/en/rf-if-and-rfid/rf-transceiver-modules-and-modems/872?k=&pkeyword=xbee&sv=0&pv1883=320634&sf=0&FV=1989%7C0%2C-8%7C872&quantity=&ColumnSort=0&page=1&stock=1&pageSize=25">
							BLE 
						</a>
						and 
						<a href="https://www.digikey.jp/products/en/rf-if-and-rfid/rf-transceiver-modules-and-modems/872?FV=1989%7C0%2C-8%7C872%2C578%7C423589&quantity=0&ColumnSort=0&page=1&stock=1&pageSize=25&pkeyword=xbee">
							RF/Zigbee
						</a>
						, one valuable source to help inform the decision was 
						<a href="https://www.researchgate.net/publication/271327094_On_the_Potential_of_Bluetooth_Low_Energy_Technology_in_Vehicular_Applications">
							this paper on the Potential of Bluetooth Low Energy
							Technology in Vehicular Applications
						</a>
						. As summarized in the Comparison between BLE and ZigBee section,
						BLE offers a faster data rate, lower latency, lower cost, and
						more robust in the presence of strong WiFi interference.
						Therefore, I recommended that we proceed with buying a BLE chip.
						<br>
						I then did some research to get a sense of what are some of the
						most popular BLE controllers. 
						<a href="https://developex.com/blog/most-popular-ble-controllers/">
							This blog
						</a>
						was one of many websites that I referenced alongside filtering
						through different bluetooth chips on the digikey website. After
						some review of the available parts, I chose the 
						<a href="https://www.digikey.jp/product-detail/en/jorjin-technologies-inc/ZB7412-00/2393-ZB7412-00CT-ND/12142962">
						ZB7412 module 
						</a>
						based on the TI CC26xx BLE controller. The main primary rationale
						behind this choice was that it had the second lowest current
						consumption (behind the DA 14580 which had no available BLE modules).

					</li>
					<li>
						<b>Confirmed ability to disable IMU (
						<a href="https://www.digikey.com/en/products/detail/bosch-sensortec/BHI160B/9674245?s=N4IgTCBcDaIBxjgWgIwAY4E5VIHIBEQBdAXyA">
							BHI160B
						</a>
						) fusion core.</b>
						<br>
						After the team ordered a few IMUs, I found that this particular IMU
						had an onboard "fusion core" that allows a client to read from
						"virtual sensors." These sensors are available via on-chip sensor
						fusion.
						<br>
						If we were not concerned about about keeping the wearable low-power,
						it would be an obvious choice to use the IC sensor fusion result to
						obtain arm orientation (angular displacement). However, this fusion
						core is responsible for around 35% of the power consumption of the
						module during "significant motion" (see Fig. 1).
						<br>
						<img src="Team/progress/zghera/img/week2/bhi160b_table_13_power_consumption.jpg" style="width:40%;height:40%;">
						<br>Fig. 1: Table 13 from BHI160B datasheet
						<br>
						Thus, the ability to disable the fusion core and perform this
						computation on the host would save up to ~1 mA which would translate
						to ~1.8mW. For reference, the analog event detector consumes 0.7mW.
						<br>
						Based on section 9.4 in the BHI160B datasheet, it appears you can
						disable the fusion core so that it will only draw ~7uA.
					</li>
					<li>
						<b>Functional Specification</b> 
						<br>
						I completed the functional description, expected use case,
						computational constraints, and functional block diagram sections
						of the report.
						<br>
						The RevEx functional block diagram I created is shown in Fig. 2.
						<br>
						<img src="Team/progress/zghera/img/week2/revex-fbd.jpg" style="width:75%;height:75%;">
						<br>Fig. 2: RevEx Functional Block Diagram
					</li>
				</ul>
			</p>
			<br>

			<h4>Week 3</h4>
			<p>
				<b>Date:</b> 9/10<br>
				<b>Total hours:</b> ~12 hours<br>
				<b>Description of design efforts:</b><br>
				<ul>
					<li>
						<b>Sensor Fusion Research</b> 
						<br>
						This week I spent much of my time researching methods to estimate 
						the users arm position from the following sensor/input sources:
						<ul>
							<li>Analog potentiometer voltage reading.</li>
							<li>3-axis gyroscope reading.</li>
							<li>3-axis accelerometer reading.</li>
							<li>VR headset position.</li>
							<li>(Configuration only) VR hand controller positions.</li>
						</ul>
						<br>
						This problem contains three sub-problems:
						<ol>
							<li>Estimate the position of the forearm relative to the upper arm.</li>
							<li>Estimate the position of the upper arm relative to the shoulder.</li>
							<li>Estimate the position of the shoulder relative to the VR headset.</li>
						</ol>
						I will discuss the proposed solutions for each below:
						<br>
						<br>
						Estimation #1 can be obtained using the position of the elbow (from
						estimation #2) and the voltage reading from the potentiometer.
						<br>
						<br>
						Skipping to #3, we can initially assume the shoulder position and
						then correct this guess during a calibration phase where the user
						is instructed to manuever the controllers in a specific way.
						<br>
						<br>
						Estimation #2 is the least trivial as the shoulder joint has 3 degrees
						of freedom. The challenge is to use the location of the shoulder
						and the IMU (located on the users upper arm) to estimate the position
						of the arm relative to the shoulder.
						<br>
						As mentioned above, our IMU does contain a sensor fusion core.
						However, in an effort to keep the system as low power as possible,
						the team decided to move the sensor fusion to the host computer.
						Thus, the primary objective was to find a sensor fusion method
						that has both high accuracy/robust and computationally inexpensive.
						<br>
						In my research so far, I have identified 2 alternative solutions:
						<ol>
							<li>
								Estimate arm attitude relative to shoulder.
								<br>
 								This would likely encompass fusing the gravitational
								vector estimate using the accelerometer (with some trigonometry)
								as well as the integrated rotational velocity from gyroscope.
								Based on initial research, a Kalman or complementary filter are 
								common choices for fusing this information. After researching the
								differences between the filters (e.g. 
								<a href="https://www.researchgate.net/publication/320726643_Comparison_of_Complementary_and_Kalman_Filter_Based_Data_Fusion_for_Attitude_Heading_Reference_System">
									Comparison of Complementary and Kalman Filter Based Data Fusion for Attitude Heading Reference System
								</a>
								), the complementary filter is the choice for attitude estimation
								in our application due to its simplicity, low computational cost,
								and comparable (although consistently worse) performance
								to the Kalman filter .
								<br>
								However, there are two issues with this approach.
								<ol>
									<li>
										Correcting "yaw" drift.
										<br>
										Without a magnetometer, there is no easy way to correct
										for estimated yaw angle as there is no gravity vector
										component we can extract from the accelerometer in that
										plane. The only counter to this point I found was in
										<a href="https://ieeexplore.ieee.org/document/6385893">
										this paper
										</a>
										. They suggest (section 3B) that we may be able to get
										around this if we assume that we don't spend a lot of
										time moving solely in this plane and then we can calculate
										the drift once the joint axis is no longer in the yaw plane 
										by "integrating the gyroscope readings and comparing
										against the joint angle." But I am not sure of how well
									  this applies for our use case.
									</li>
									<li>
										Linear acceleration.
										<br>
										 In all the papers/docs I read (e.g. see end of section 2 in
										 <a href="https://www.nxp.com/docs/en/application-note/AN3461.pdf"></a>
										 this document
										 ), they all ensure "smooth movements" or just flat out
										 assume no linear acceleration. This is not true for us.
										 And it makes sense why this is a problem. How would you
										 know the difference between linear acceleration and a
										 change in the gravitational vector components (unless
										 the magnitude is greater than ~1g of course). In this case,
										 we may not be able to reliably use the accelerometer to
										 correct for gyroscope drift at all times.
									</li>
								</ol>
							</li>
							<li>
								Estimate arm pose relative to shoulder.
								<br>
								While similar to the previous point, this involves estimating 
								the absolute IMU position using the accelerometer and gyroscope.
								The advantage here is reducing the need for a magnetometer and
								being able to more robustly handle linear acceleration of the 
								arm.
								<br>
								The main downside here is that we would need a more complex
								fusion algorithm such as the
								<a href="chrome-extension://cbnaodkpfinfiipjblikofhlhlcickei/src/pdfviewer/web/viewer.html?file=file:///C:/Users/zghera/Downloads/sensors-17-01257.pdf">
								EKF
								</a>
								. As a result, I would need to
								perform much more research on how to implement something like an
								EKF properly for our use case.
							</li>
						</ol>
						<br>
						After analysis of the alternatives, I determined that implementing
						and tuning our own sensor fusion algorithm would be time consuming 
						and was unnecessary as the sensor fusion core on our IMU was able to
						perform sensor fusion with a lower power cost associated.
						<br>
						<br>
						However, after doing some research on compatable magnetometer with
						the BHI160B, the only available magnetometers were very small 
						(1.5 x 1.5 mm) BGA chips that would be impossible to solder with 
						our current equipment.
						<br>
						However, I still believed that using an existing implementation to
						the sensor fusion (whether that be in software or hardware) made
						more sense for our project given the limited time and resources.
						<br>
						I helped search for a new IMU that had a built in magnetometer. We found the 
						<a href="https://www.sparkfun.com/products/15335">
						ICM-20948
						</a>
						It met all of our criteria (e.g. low power, >100 Hz update rate,
						easy to solder, etc.). 
						<br>
						<br>
						There were two options for sensor fusion with this IMU:
						<ol>
							<li>
								A built in FPGA for digital motion processing (DMP). Swag found an 
								<a href="https://github.com/ZaneL/Teensy-ICM-20948">
								Arduino library on GitHub 
								</a>
								that uses the IMU company's (Invensense) firmware for interfacing with the 
								DMP FPGA.
							</li>
							<li>
								An open source 9DOF IMU sensor fusion implementation such as
								<a href="https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/">
									x-io technologies IMU algorithms
								</a>
								.
							</li>
						</ol>
						After some poking around in both of the code repositories and
						doing some additonal research, I determined that the easiest path
						would be option 2. According to
						<a href="https://github.com/pimoroni/icm20948-python/issues/1#issuecomment-539730082">
						this GitHub issue comment 
						</a>
						, it was a challenge to interface with Invensense's FPGA. For example,
						you must register and get permission from Invensese to down load the
						proprietary binary image before you can use the FPGA. Additionally, performing
						the sensor fusion on host allows for easier modification and debugging.
						<br>
					</li>
					<li>
						<b>Software Overview</b> 
						<br>
						My other main task this week was to complete the software overview 
						document. I worked with Isaac and to determine some details such as the
						bluetooth communication and the microcontroller states. Outside of this 
						collaboration, I was responsible for writing the software overview 
						and working	through all the software design decisions encapsulated
						in the document.
						<br>
						The RevEx host computer flowchart I created is shown in Fig. 3.
						<br>
						<img src="Team/progress/zghera/img/week3/revex-flowchart-host-computer.jpg" style="width:85%;height:85%;">
						<br>Fig. 3: Host computer flowchart
					</li>
				</ul>
			</p>		
			<br>

			<h4>Week 4</h4>
			<p>
				<b>Date:</b> 9/17<br>
				<b>Total hours:</b> ~4 hours<br>
				<b>Description of design efforts:</b><br>
				I was unable to spend much time on senior design this week as I had
				big asssignments due in three of my other classes.
				<br>
				My time was split between the three tasks listed below:
				<ul>
					<li>
						<b>Digitization of hand-drawn signal flow diagrams for Electrical Overview.</b>
						<br>
						I transfered Swagat's initial hand-drawn sigal flow diagrams for the top-level system
						as well as the data aquisition and haptic feedback subsystems.
						The signal flow diagram for the haptic feedback system is shown in Fig. 4 below.
						<img src="Team/progress/zghera/img/week4/HMmS-Signal-Flow-Diagram.jpg" style="width:85%;height:85%;">
						<br>Fig. 4: Haptic feedback signal flow diagram
					</li>
					<li>
						<b>Assist with Bill of Materials</b>
						<br>
						I filled out the details for around 1/3 of our components in the Bill of Materials
						assignment.
					</li>
					<li>
						<b>Testing Host Computer Bluetooth Integration with Unreal Engine</b>
						The next step for the host computer software is to determine the best method 
						for bluetooth communication in conjuction with the rest of the VR system.
						We have yet to decide whether we will use Unreal or Unity as our VR engine.
						Matthew has a lot of experience working with Unity, so I thought that it would
						be good for me to get some exposure to Unreal Engine before we make our decision.
						<br>
						I went through the initial VR development tutorial for Unreal and did some research
						on the common practices for integrating bluetooth into a VR desktop application. One
						of the most popular methods is to create a seperate process using
						<a href="https://github.com/pimoroni/icm20948-python/issues/1#issuecomment-539730082">
							Windows Socket bluetooth library
						</a>
						that the VR simulation can subscribe to. I was able to create a sample class that
						initializes the Windows Socket DLL and run the initial VR sandbox scene (see Fig. 5).
						<img src="Team/progress/zghera/img/week4/unreal-example.jpg" style="width:100%;height:100%;">
						Fig 5: Unreal Engine VR Sandbox Scene with Successful Bluetooth Initialization
					</li>
				</ul>
			</p>
			<br>

			<h4>Week 5</h4>
			<p>
				<b>Date:</b> 9/24<br>
				<b>Total hours:</b> ~8 hours<br>
				<b>Description of design efforts:</b><br>
				This week, my efforts focused on the host-side BLE interface and the VR
				simulation.
				<br>
				After my experience working in Unreal last week, I finally got a taste 
				of how little I know about game development and game engines. That being
				said, I talked with Mattew and decided that choosing Unity as our game
				engine would make much more sense as Matt has 3+ years of experience developing
				VR applications in Unity. Thus, developing in Unity	would speed up my
				learning process a lot as Matt can answer any questions I have. 
				<br>
				Having decided to use Unity, I did some research to determine the simplest
				way to integrate BLE communication into a Unity application. The main problem
				is that the Unity editor cannot access the .Net-Core library needed for 
				the Windows BLE library (UWP API) (see this
				<a href="https://stackoverflow.com/questions/60686302/ble-device-with-unity/65403179">
					Stack Overflow Post
				</a>
				and this
				<a href="https://mtaulty.com/2019/03/22/rough-notes-on-experiments-with-uwp-apis-in-the-unity-editor-with-c-winrt/">
					blog post
				</a>
				for more information). The typical solution is to call the UWP API from a C++
				application and then create a dll that components in Unity can use.
				<br>
			  Due to the popularity of using BLE in Unity projects, people have released
				open source code to interact with the UWP BLE API. I read the code and 
				tested a few of the open source implementations and found one that will
				suite our needs well: 
				<a href="https://github.com/adabru/BleWinrtDll">Adam Brunnmeier's BleWinrtDll</a>
				. See Fig. 6 for a screenshot of the BleWinrtDll sample demo I tested in
				Unity from within my appartment.
				<br>
				<img src="Team/progress/zghera/img/week5/unity-ble-demo.jpg" style="width:75%;height:75%;">
				<br>Fig. 6: Unity BleWinrtDll Demo
				<br>
				<br>
				Now that we have a easy way to connect and read data over BLE, the next step
				for me was to get some fundamentals in Unity development. At the same time, 
				testing the sensor fusion algorithm identified in week 3 (see above) is a
				critical step. However Isaac is still working on communication between 
				the MCU and our BLE chip. So we decided that in the meantime it would be
				beneficial to send the raw IMU data over serial from an Arduino to the
				host-computer. 
				<br>
				To accomplish both goals, I started developing a Unity scene that reads 
				data from a serial port and displays the raw IMU data in a 2D dashboard (
				see Fig. 7). As part of this, I am reading/watching a lot of basic 
				Unity tutorials and asking Matthew a lot of questions as I develop this 
				initial prototype scene and try to get up to speed with Unity.
				<br>
				<img src="Team/progress/zghera/img/week5/unity-imu-data-display.jpg" style="width:95%;height:95%;">
				<br>Fig. 7: Unity IMU Data Serial Display
			</p>
			<br>

			<h4>Week 6</h4>
			<p>
				<b>Date:</b> 10/1<br>
				<b>Total hours:</b> ~12 hours<br>
				<b>Description of design efforts:</b><br>
				This week was soely focused on development and testing of the serial
				communication interface transmitting the IMU data from the Arduino
				to the host computer. Although we will be using the STM32 and BLE for
				transmitting sensor data, Isaac is starting to realize that interfacing
				with the BLE and IMU chips is non-trivial. Thus, the importance of this
				serial communication is critical to facilitate development of the software
				on the host computer.
				<br>
				I need live sensor data to test the sensor fusion and other aspects of the
				VR simulation while Isaac works on the interfacing with the IMU and BLE with
				the STM MCU. Once Isaac is ready for BLE, all I have to do is swap in the
				serial tranceiver object with an analogous BLE tranceiver object. Additionally,
				if there are ever any problems with the STM MCU once BLE and IMU interfacing is
				complete, I can still continue testing the host computer with the serial interface.
				<br><br>
				Isaac found an 
				<a href="https://github.com/sparkfun/SparkFun_ICM-20948_ArduinoLibrary">
				Arduino library for the IMU chip
				</a>
				and used the 
				<a href="https://github.com/sparkfun/SparkFun_ICM-20948_ArduinoLibrary/blob/main/examples/Arduino/Example5_DualSPITest/Example5_DualSPITest.ino">
				dual SPI test script
				</a>
				as a starting point. After Isaac modified the script such that we only used a single SPI
				line with our IMU, I wrote some Arduino code to write IMU packets (based on the structure
				defined in the software overview) over the serial line (See Fig. 8).
				<br>
				As of now, `floatToBytes()` simply `memcpy`s the 16 bit float into the byte buffer. However,
				depending on architecutre of the chip the Arduino uses, I may need to do some additional
				manipulation so that the host can properly decode the two bytes into a 16-bit float.
				<br><br>
				<img src="Team/progress/zghera/img/week6/arduino-imu-serial.jpg" style="width:50%;height:50%;">
				<br>Fig. 8: Arduino Script to Send IMU Packer over Serial Line
				<br>
				<br>
				I also wrote the serial interface on the host-side from using the Windows SerialPort
				library and C# threads (see Fig. 9 and Fig. 10). 

				<div style="display: flex;align-items: flex-start;">
					<div style="flex: 50%;padding: 5px;">
						<img src="Team/progress/zghera/img/week6/host-imu-serial1.jpg" style="width:100%;height:100%;">
						<br>Fig. 9: SerialReader Class that is Responsible for Reading and Decoding IMU Packets Received on the Host Computer.
					</div>
					<div style="flex: 50%;">
						<img src="Team/progress/zghera/img/week6/host-imu-serial2.jpg" style="width:100%;height:100%;">
						<br>Fig. 10: Other Classes Used by the SerialReader Class on the Host Computer.
					</div>
				</div>
				<br><br>

				I has some initial success with this implementation this week. As seen in Fig. 11, the IMU
				data is successfully received on the host computer. While this is a great first step, there
				are multiple areas for improvement before I can move on to implementing and testing sensor fusion:
				<ol>
					<li><b>Thread reader vs. Packet received event handler</b>
						<br>
						There is a build in event handler method build into the SerialPort class using C# delegates.
						I initially tried this approach but the event was never triggered so it appeared that no 
						packets were received on the host. I want to see if I can get this method working as it is
						more readable, efficient, and less error prone than manually manipulating threads and
						packet retreival.
					</li>
					<li><b>Filter out IMU Start Message from Packet Retreival</b>
						<br>
						As seen in Fig. 11, the first thing the Arduino sends over serial is an initialization
						string from the IMU. I need to be able to filter this out on the host such that
						the the data packets are aligned correctly (i.e. don't let chars from init message sneak
						into the data packets).
					</li>
					<li><b>Arduino packet send at 100 Hz</b>
						<br>
						Because we will be sending over bluetooth at 100 Hz, we should replicate this in the 
						serial send interface as well.
					</li>
					<li><b>Update UI in Unity</b>
						<br>
						As seen in Fig. 7 of week 5, I created a toy UI. The final step once all the communications
						are working properly is to update this UI to help get me more familiar with interacting with
						Unity game objects and have a prettier way to view current IMU data before implementing
						the sensor fusion and associated scene.
					</li>
				</ol>

				<br><br>
				<img src="Team/progress/zghera/img/week6/imu-data-received-in-unity.jpg" style="width:45%;height:45%;">
				<br>Fig. 11: Unity Terminal Output When Running Both the Arduino and Host-Computer IMU Packet Transmit Code.
				<br>
				<br>

			</p>
			<br>


			<br>
		</div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
